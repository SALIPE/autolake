- name: Ensure Java JDK is installed
  apt:
    name: openjdk-11-jdk
    state: present
    update_cache: yes

- name: Create spark user
  user:
    name: spark
    shell: /bin/bash
    create_home: yes

- name: Download Spark
  get_url:
    url: https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
    dest: /tmp/spark-3.2.1-bin-hadoop3.2.tgz

- name: Unarchive Spark
  unarchive:
    src: /tmp/spark-3.2.1-bin-hadoop3.2.tgz
    dest: /opt/
    remote_src: yes

- name: Create symlink for Spark
  file:
    src: /opt/spark-3.2.1-bin-hadoop3.2
    dest: /opt/spark
    state: link

- name: Set environment variables for Spark
  copy:
    dest: /etc/profile.d/spark.sh
    content: |
      export SPARK_HOME=/opt/spark
      export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
    mode: '0644'

- name: Download Hadoop AWS dependency
  get_url:
    url: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.2/hadoop-aws-3.3.2.jar
    dest: /opt/spark/jars/hadoop-aws-3.3.2.jar
    mode: '0644'

- name: Download AWS SDK bundle dependency
  get_url:
    url: https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar
    dest: /opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar
    mode: '0644'

- name: Ensure Spark directories exist
  file:
    path: "{{ item }}"
    state: directory
    owner: spark
    group: spark
    mode: '0755'
  loop:
    - /opt/spark
    - /opt/spark/logs
    - /opt/spark/work
    - /opt/spark/conf
  tags:
    - spark

- name: Ensure correct permissions on /opt/spark
  file:
    path: /opt/spark
    state: directory
    owner: spark
    group: spark
    recurse: yes
    mode: '0755'
  tags:
    - spark

- name: Create Spark Master systemd service
  copy:
    dest: /etc/systemd/system/spark-master.service
    content: |
      [Unit]
      Description=Apache Spark Master
      After=network.target

      [Service]
      Type=forking
      User=spark
      Group=spark
      Environment="SPARK_HOME=/opt/spark"
      Environment="SPARK_MASTER_HOST=192.168.122.232"
      ExecStart=/opt/spark/sbin/start-master.sh
      ExecStop=/opt/spark/sbin/stop-master.sh
      Restart=on-failure

      [Install]
      WantedBy=multi-user.target

- name: Create Spark Worker systemd service
  copy:
    dest: /etc/systemd/system/spark-worker.service
    content: |
      [Unit]
      Description=Apache Spark Worker
      After=network.target

      [Service]
      Type=forking
      User=spark
      Group=spark
      Environment="SPARK_HOME=/opt/spark"
      Environment="SPARK_MASTER_HOST=192.168.122.232"
      ExecStart=/opt/spark/sbin/start-worker.sh spark://192.168.122.232:7077
      ExecStop=/opt/spark/sbin/stop-worker.sh
      Restart=on-failure

      [Install]
      WantedBy=multi-user.target

- name: Enable and start Spark Master
  systemd:
    name: spark-master
    enabled: yes
    state: started
    daemon_reload: yes

- name: Enable and start Spark Worker
  systemd:
    name: spark-worker
    enabled: yes
    state: started
    daemon_reload: yes
